{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "📌 **This notebook has been updated in [jhj0517/finetuning-notebooks](https://github.com/jhj0517/finetuning-notebooks) repository!**\n",
        "\n",
        "## Version : 1.0.0\n",
        "---"
      ],
      "metadata": {
        "id": "doKhBBXIfS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #(Optional) Check GPU\n",
        "\n",
        "#@markdown To train SDXL lora at least 12GB VRAM is recommended.\n",
        "#@markdown <br> And you need at least 16GB for CPU RAM, which is unfortunately not available on the free tier in Colab.\n",
        "#@markdown <br>You can check your GPU setup before start.\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "23yZvUlagEsx",
        "cellView": "form",
        "outputId": "fcb2710d-127d-44f0-8df2-f04ab026e090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 24 20:22:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNbSbsctxahq",
        "cellView": "form",
        "outputId": "2b51e43b-9a96-48ef-e16d-4e8b998e48fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 82027, done.\u001b[K\n",
            "remote: Counting objects: 100% (22623/22623), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1498/1498), done.\u001b[K\n",
            "remote: Total 82027 (delta 21938), reused 21125 (delta 21125), pack-reused 59404 (from 3)\u001b[K\n",
            "Receiving objects: 100% (82027/82027), 58.49 MiB | 13.05 MiB/s, done.\n",
            "Resolving deltas: 100% (60560/60560), done.\n",
            "/content/diffusers\n",
            "Obtaining file:///content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (8.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.33.0.dev0) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.33.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.33.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.33.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.33.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.33.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers==0.33.0.dev0) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.33.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.33.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.33.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.33.0.dev0) (2025.1.31)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building editable for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.33.0.dev0-0.editable-py3-none-any.whl size=11241 sha256=9da63ac24bbfd195b0c77343c9ebc97824c5c8215fa9ebb7af8c41c65ace070e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y82r11ov/wheels/30/15/ca/ab6e88c89d6ba7047b3f155894c6c346e7cf06067fd132ae62\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.32.2\n",
            "    Uninstalling diffusers-0.32.2:\n",
            "      Successfully uninstalled diffusers-0.32.2\n",
            "Successfully installed diffusers-0.33.0.dev0\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n"
          ]
        }
      ],
      "source": [
        "#@title #1. Install Dependencies\n",
        "#@markdown This notebook is powered by https://github.com/huggingface/diffusers\n",
        "!git clone https://github.com/huggingface/diffusers\n",
        "%cd diffusers\n",
        "!pip install -e .\n",
        "\n",
        "# Cherry picked dependencies from https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/requirements_sdxl.txt to use in Colab.\n",
        "!pip install ftfy\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes\n",
        "# Only install this if you want to use optimization with xformers.\n",
        "# !pip install xformers\n",
        "\n",
        "\n",
        "# Comment on the requirements above, and uncomment below if you're not using Colab.\n",
        "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
        "# !pip install deepspeed\n",
        "# !pip install accelerate>=0.22.0\n",
        "# !pip install transformers>=4.25.1\n",
        "# !pip install ftfy\n",
        "# !pip install tensorboard\n",
        "# !pip install Jinja2\n",
        "# !pip install datasets\n",
        "# !pip install peft==0.7.0\n",
        "# !pip install xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 2. (Optional) Mount Google Drive\n",
        "\n",
        "#@markdown It's not mandatory but it's recommended to mount to Google Drive and use the Google Drive's path for your training image dataset.\n",
        "\n",
        "#@markdown The dataset should have following structure:\n",
        "\n",
        "#@markdown This notebook uses diffuser's dreambooth LoRA training, you only need image files in the dataset with this way.\n",
        "\n",
        "#@markdown ### Example File Structure (Image Files Only):\n",
        "#@markdown ```\n",
        "#@markdown your-dataset/\n",
        "#@markdown ├── a (1).png         # Image file\n",
        "#@markdown ├── a (2).png         # Another image file\n",
        "#@markdown ├── a (3).png         # Another image file\n",
        "#@markdown ```\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M1bu3MpsACOu",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a179c7-8249-43d2-bca3-256a36cf96b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 3. (Optional) Register Huggingface Token To Download Base Model\n",
        "\n",
        "#@markdown If you don't have entire base model files ([stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main)) in the drive you need to sign in to Huggingface to download the model.\n",
        "\n",
        "#@markdown Get your tokens from https://huggingface.co/settings/tokens, and register it in colab's seceret as **`HF_TOKEN`** and use it in any notebook. ( 'Read' permission is enough )\n",
        "\n",
        "#@markdown To register secrets in colab, click on the key-shaped icon in the left panel and enter your **`HF_TOKEN`** like this:\n",
        "\n",
        "#@markdown ![image](https://media.githubusercontent.com/media/jhj0517/finetuning-notebooks/master/docs/screenshots/colab_secrets.png)\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"HF_TOKEN environment variable has been set.\")"
      ],
      "metadata": {
        "id": "9WzQRwZij5jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900c2f41-f2df-47f7-85ef-237b435cede2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF_TOKEN environment variable has been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # 4. Train with Parameters\n",
        "import os\n",
        "import toml\n",
        "import json\n",
        "import re\n",
        "\n",
        "#@markdown ## Paths Configuration\n",
        "DATASET_DIR = \"/content/drive/MyDrive/finetuning-notebooks/dataset/dog-dreambooth\" # @param {type:\"string\"}\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/finetuning-notebooks/sdxl/outputs\" # @param {type:\"string\"}\n",
        "OUTPUT_NAME = \"My-SDXL-LoRA-V1\" # @param {type:\"string\"}\n",
        "\n",
        "OUTPUT_DIR = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#@markdown ## Base Model Configuration\n",
        "BASE_MODEL_PATH_OR_ID = \"stabilityai/stable-diffusion-xl-base-1.0\" # @param {type:\"string\"}\n",
        "BASE_VAE_PATH_OR_ID = \"madebyollin/sdxl-vae-fp16-fix\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Dataset Configuration\n",
        "# CAPTION_EXTENSION = \".txt\" # @param {type:\"string\"}\n",
        "RESOLUTION = 1024 # @param {type:\"integer\"}\n",
        "# CAPTION_COLUMN = \"text\"\n",
        "\n",
        "#@markdown ## Training Settings\n",
        "MIXED_PRECISION = \"bf16\" # @param [\"no\", \"fp16\", \"bf16\"]\n",
        "INSTANCE_PROMPT = \"A sks dog\" # @param {type:\"string\"}\n",
        "RANDOM_FLIP = True # @param {type:\"boolean\"}\n",
        "TRAIN_BATCH_SIZE = 1 # @param {type:\"integer\"}\n",
        "MAX_TRAIN_STEPS = 500 # @param {type:\"integer\"}\n",
        "CHECKPOINTING_STEPS = 1 # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 1e-4 # @param {type:\"number\"}\n",
        "LR_SCHEDULER = \"constant\" # @param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
        "LR_WARMUP_STEPS = 0 # @param {type:\"integer\"}\n",
        "GRADIENT_ACCUMULATION_STEPS = 4 # @param {type:\"integer\"}\n",
        "SEED = 77 # @param {type:\"integer\"}\n",
        "GRADIENT_CHECKPOINTING = True # @param {type:\"boolean\"}\n",
        "USE_8_BIT_ADAM = True # @param {type:\"boolean\"}\n",
        "# ENABLE_XFORMERS_MEMORY_EFFICIENT_ATTENTION = False # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown ## Network Settings\n",
        "RANK = 4 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown ## Validation Configuration\n",
        "#@markdown WandB is a 3rd party service, to use it you need to get an API key from https://wandb.ai/authorize.\n",
        "ENABLE_WANDB = False # @param {type:\"boolean\"}\n",
        "VALIDATION_PROMPT = \"A sks dog is playing with a ball in grass\"  # @param {type:\"string\"}\n",
        "# NUM_VALIDATION_IMAGES = 4 # @param {type:\"integer\"}\n",
        "VALIDATION_EPOCHS = 25 # @param {type:\"integer\"}\n",
        "\n",
        "# Write Command\n",
        "command_parts = [\n",
        "    \"accelerate\", \"launch\",\n",
        "    \"\\\"/content/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py\\\"\",\n",
        "]\n",
        "\n",
        "command_parts.extend([\n",
        "    f\"--pretrained_model_name_or_path=\\\"{BASE_MODEL_PATH_OR_ID}\\\"\",\n",
        "    f\"--pretrained_vae_model_name_or_path=\\\"{BASE_VAE_PATH_OR_ID}\\\"\",\n",
        "    f\"--instance_data_dir=\\\"{DATASET_DIR}\\\"\",\n",
        "    f\"--instance_prompt=\\\"{INSTANCE_PROMPT}\\\"\",\n",
        "#    f\"--caption_column={CAPTION_COLUMN}\",\n",
        "    f\"--mixed_precision={MIXED_PRECISION}\",\n",
        "    f\"--resolution={RESOLUTION}\",\n",
        "    f\"--max_train_steps={MAX_TRAIN_STEPS}\",\n",
        "    f\"--train_batch_size={TRAIN_BATCH_SIZE}\",\n",
        "    f\"--checkpointing_steps={CHECKPOINTING_STEPS}\",\n",
        "    f\"--learning_rate={LEARNING_RATE}\",\n",
        "    f\"--lr_scheduler={LR_SCHEDULER}\",\n",
        "    f\"--lr_warmup_steps={LR_WARMUP_STEPS}\",\n",
        "    f\"--seed={SEED}\",\n",
        "    f\"--output_dir={OUTPUT_DIR}\",\n",
        "    f\"--validation_prompt=\\\"{VALIDATION_PROMPT}\\\"\",\n",
        "#    f\"--num_validation_images={NUM_VALIDATION_IMAGES}\",\n",
        "    f\"--validation_epochs={VALIDATION_EPOCHS}\",\n",
        "    f\"--gradient_accumulation_steps={GRADIENT_ACCUMULATION_STEPS}\",\n",
        "    f\"--rank={RANK}\",\n",
        "\n",
        "])\n",
        "\n",
        "if RANDOM_FLIP:\n",
        "    command_parts.append(\"--random_flip\")\n",
        "\n",
        "if ENABLE_WANDB:\n",
        "    command_parts.append(\"--report_to=\\\"wandb\\\"\")\n",
        "\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "    command_parts.append(\"--gradient_checkpointing\")\n",
        "\n",
        "if USE_8_BIT_ADAM:\n",
        "    command_parts.append(\"--use_8bit_adam\")\n",
        "\n",
        "# if ENABLE_XFORMERS_MEMORY_EFFICIENT_ATTENTION:\n",
        "#     command_parts.append(\"--enable_xformers_memory_efficient_attention\")\n",
        "\n",
        "# Write metadata.jsonl for the dataset\n",
        "def create_metadata_jsonl(dataset_dir, caption_extension=\".txt\"):\n",
        "    metadata = []\n",
        "    image_files = [f for f in os.listdir(dataset_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        caption_file = f\"{base_name}{caption_extension}\"\n",
        "\n",
        "        if os.path.exists(os.path.join(dataset_dir, caption_file)):\n",
        "            try:\n",
        "                with open(os.path.join(dataset_dir, caption_file), \"r\", encoding=\"utf-8\") as f:\n",
        "                    caption = f.read().strip()\n",
        "\n",
        "                match = re.search(r\"\\((\\d+)\\)\", base_name)\n",
        "                if match:\n",
        "                    file_number = int(match.group(1))\n",
        "                    new_file_name = f\"{file_number:04d}.png\"\n",
        "                else:\n",
        "                    file_number = len(metadata) + 1\n",
        "                    new_file_name = f\"{file_number:04d}.png\"\n",
        "\n",
        "                metadata.append({\"file_name\": new_file_name, \"text\": caption})\n",
        "\n",
        "                os.rename(os.path.join(dataset_dir, image_file), os.path.join(dataset_dir, new_file_name))\n",
        "                os.rename(os.path.join(dataset_dir, caption_file), os.path.join(dataset_dir, f\"{file_number:04d}{caption_extension}\"))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"Warning: Caption file {caption_file} not found for {image_file}\")\n",
        "\n",
        "    metadata_path = os.path.join(dataset_dir, \"metadata.jsonl\")\n",
        "    with open(metadata_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for item in metadata:\n",
        "            json.dump(item, outfile, ensure_ascii=False)\n",
        "            outfile.write(\"\\n\")\n",
        "\n",
        "# Diffuser's script does not use each caption with dreambooth.\n",
        "# create_metadata_jsonl(DATASET_DIR, CAPTION_EXTENSION)\n",
        "# print(f\"{os.path.join(DATASET_DIR, 'metadata.jsonl')} has written.\")\n",
        "\n",
        "# Train\n",
        "!accelerate config default\n",
        "command = \" \".join(command_parts)\n",
        "print(command)\n",
        "!{command}"
      ],
      "metadata": {
        "id": "fob2cRMQeW5C",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # 5. (Optional) Test your LoRA\n",
        "\n",
        "from huggingface_hub.repocard import RepoCard\n",
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "BASE_MODEL_PATH_OR_ID = \"stabilityai/stable-diffusion-xl-base-1.0\" # @param {type:\"string\"}\n",
        "YOUR_LORA_PATH = \"/content/drive/MyDrive/finetuning-notebooks/sdxl/outputs/something/pytorch_lora_weights.safetensors\" # @param {type:\"string\"}\n",
        "PROMPT = \"A picture of a sks dog in a bucket\" # @param {type:\"string\"}\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(BASE_MODEL_PATH_OR_ID, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.load_lora_weights(YOUR_LORA_PATH)\n",
        "image = pipe(PROMPT, num_inference_steps=25).images[0]\n",
        "image.save(\"sks_dog.png\")\n",
        "\n",
        "from IPython.display import display\n",
        "display(image)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PALmQfvtSk6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}